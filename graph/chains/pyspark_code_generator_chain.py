# chain that takes the output of the grading step and generates PySpark code

from os import system
from typing import List
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from pydantic import BaseModel, Field
from graph.chains.prompts.pyspark_code_generator_prompt import pyspark_code_generator_simple_prompt, \
    llm_pyspark_generator_with_evaluator_prompt
from graph.chains.prompts.variable_name_extractor_prompt import llm_variable_name_extractor_prompt

llm = ChatOpenAI(temperature=0.8, model_name="gpt-5", reasoning_effort='high',
                 verbosity='high')


class GeneratePysparkCode(BaseModel):
    """
    Schema for generating and extracting important variable names from PySpark code.
    """
    pyspark_code: str = Field(
        description=(
            "A single block of runnable PySpark code that performs the requested transformation "
            "using the available input data. Do not include markdown backticks (```) or anything apart from runnable code"
        )
    )
    # output_names: List[str] = Field(
    #     description=(
    #         "List of the final, resulting DataFrame or variable names created by "
    #         "the 'pyspark_code'. These variables represent the output of the "
    #         "current processing step and will be required for the *next* step in the data pipeline."
    #     )
    # )

    # remarks: List[str] = Field(
    #     description=(
    #   		"Important remarks for human reference. If code cant be generated, give a one line reason why"
    #     )
    # )
    
structured_llm_coder = llm.with_structured_output(GeneratePysparkCode)

# TODO: incorporate these in system prompt and generator_prompt
# - generator_llm_remarks: 
#     - Remarks from previous steps by llm that is generating code, can be empty
#     - Take this history into consideration while improving on current code if non empty
# - debugger_llm_remarks:
#     - Remarks from previous steps by llm that is checking the generated code, can be empty
#     - Take this history into consideration while improving on current code if non empty
# - human_evaluator_remarks:
    # - Remarks from previous steps by human that is evaluating the generated code, can be empty
    # - Incorporating this feedback is critical
    # - Take this history into consideration while improving on current code if non empty
# - pyspark_code_raw:
#     - Codes generated by llm in previous steps
#     - Each iteration learns from provided feedback and improves code. Learn what needs to be changed from this history

# system = pyspark_code_generator_simple_prompt
system = llm_pyspark_generator_with_evaluator_prompt

# generator_prompt = ChatPromptTemplate.from_messages(
#     [
#         ("system", system),
#         ("human", """: \n\n Relevant coulum details are {required_columns_data} \n\n User question: {question} 
#          ),
#     ]
# )

generator_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system),
        ("human", """: \n\n Relevant coulum details are {required_columns_data} \n\n User question: {question} 
         \n\n Previous code: {previous_code} \n\n Previous remarks: {previous_remarks}. 
         If Previous remarks is provided, strictly incrorporate it as feedback while generating code."""),
    ]
)

pyspark_code_generator = generator_prompt | structured_llm_coder

#################################################################################################################

# Variable name extractor chain to extract output variable names from generated code

class GenerateVariableNames(BaseModel):
    output_names: List[str] = Field(
        description=(
            "List of the final, resulting DataFrame or variable names created by "
            "the 'pyspark_code'. These variables represent the output of the "
            "current processing step and will be required for the *next* step in the data pipeline."
        )
    )

structured_llm_variable_name_extractor = llm.with_structured_output(GenerateVariableNames)

system_variable_name_extractor = llm_variable_name_extractor_prompt

variable_name_extractor_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_variable_name_extractor),
        ("human", ": \n\n Generated PySpark code: {pyspark_code}, \n\n User question: {question}"),
    ]
)

variable_name_extractor = variable_name_extractor_prompt | structured_llm_variable_name_extractor

