# This node generates pyspark code based on the graded retrievals from the previous step
import pandas as pd
from typing import Any, Dict, List
from graph.state import SubQueryState
from graph.chains.pyspark_code_generator_chain import pyspark_code_generator

relational_db = pd.read_parquet("data/relational_database.parquet")


def pyspark_code_generator_node(state: SubQueryState) -> Dict[str, Any]:
    print("---PYSPARK CODE GENERATION---")
    # extract graded columns segments from state
    graded_columns_segments = state["graded_columns_segments"]

    graded_columns_segments_yes = graded_columns_segments.get("yes", [])
    column_details_list = []
    for column_metadata in graded_columns_segments_yes:
        hash_id = column_metadata['hash_id']
        column_row = relational_db[relational_db['hash_id'] == hash_id]
        # convert the row to a string representation
        column_row = column_row.to_dict(orient='records')[0]
        column_details_list.append(column_row)
    
    # extract simple question from state
    simple_question = state["simple_question"]

    # extract latest pyspark code from state (if any)
    pyspark_code_raw = state.get("pyspark_code_raw", [])
    latest_pyspark_code = pyspark_code_raw[-1] if pyspark_code_raw else ""

    # extract latest remarks from state (if any)
    evaluator_llm_remarks = state.get("code_evaluation_remarks", [])
    latest_evaluator_remarks = evaluator_llm_remarks[-1] if evaluator_llm_remarks else ""

    # print("Columns details for code generation: ", column_details_list)
    
    # TODO : add llm, debugger, human remarks and code history here
    # prepare input for the pyspark code generator chain
    input_data = {
        "required_columns_data": column_details_list,
        "question": simple_question,
        "previous_code": latest_pyspark_code,
        "previous_remarks": latest_evaluator_remarks
    }

    response = pyspark_code_generator.invoke(input=input_data)

    # pyspark_generated_code: str = response["pyspark_code"]   # the code generated by llm
    # output_names: List[str] = response["output_names"]  # Names of the outputs that the code is supposed take as generate
    # llm_remarks: List[str] = response["llm_remarks"]  # Remarks regarding code, generated by llm

    pyspark_generated_code: str = response.pyspark_code   
    # output_names: List[str] = response.output_names  
    # llm_remarks: List[str] = response.remarks

    # TODO: for now we are appending the pyspark_code_raw with a list containing the generated code string everytime
    # This is a desgin decision, we need to do exec(state['pyspark_code_raw'][-1]) in the final execution node 
    # We can shift this to overwriting the list to save memory overhead, but current helps in debugging, reasoning
    
    # return {
    # "pyspark_code_raw": state.get("pyspark_code_raw", []) + [str(pyspark_generated_code)],
    # "output_variable_names_raw": state.get("output_variable_names_raw", []).append(output_names),
    # "generator_llm_remarks": state.get("output_variable_names_raw", []).append(llm_remarks),
    #     }

    # Pass to human code check node
    return {
    "pyspark_code_raw": state.get("pyspark_code_raw", []) + [str(pyspark_generated_code)]
        }
    # return {"pyspark_code_raw": [str(pyspark_generated_code)]}

